{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa56e64",
   "metadata": {},
   "source": [
    "The code comes from a partially modified version of the SimAlign script available here: https://github.com/cisnlp/simalign/blob/master/simalign/simalign.py\n",
    "\n",
    "The UGARIT/grc-alignment model (https://huggingface.co/UGARIT/grc-alignment), a XML-ROBERTa-based language model, fine-tuned for the automatic alignment of multilingual texts at the word level.\n",
    "\n",
    "The projection consists in the transfer of the loc_id from the English token to the Latin token. IOB labels are not projected since the position of the tokens can differ between the English and the Latin sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265efabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d55217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, XLMRobertaModel, XLMRobertaTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"UGARIT/grc-alignment\"\n",
    "device=torch.device('cpu')\n",
    "layer: int=8\n",
    "distortion: float = 0.0\n",
    "\n",
    "config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
    "emb_model = AutoModelForMaskedLM.from_pretrained(model, config=config)\n",
    "emb_model.eval()\n",
    "emb_model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the function to get the word embeddings\n",
    "def get_embed_list(sent_batch) -> torch.Tensor:\n",
    "        if emb_model is not None:\n",
    "            with torch.no_grad():\n",
    "                if not isinstance(sent_batch[0], str):\n",
    "                    inputs = tokenizer(sent_batch, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\") ## tokenize the sentence\n",
    "                else:\n",
    "                    inputs = tokenizer(sent_batch, is_split_into_words=False, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                hidden = emb_model(**inputs.to(device))[\"hidden_states\"]  ## create the embeddings\n",
    "                if layer >= len(hidden):\n",
    "                    raise ValueError(f\"Specified to take embeddings from layer {layer}, but model has only {len(hidden)} layers.\")\n",
    "                outputs = hidden[layer]\n",
    "                return outputs[:, 1:-1, :]\n",
    "        else:\n",
    "            return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the function to get the similarity matrix\n",
    "def get_similarity(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    return (cosine_similarity(X, Y) + 1.0) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68bb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the function to calculate the itermax matching method\n",
    "def iter_max(sim_matrix: np.ndarray, max_count: int=2) -> np.ndarray:\n",
    "    alpha_ratio = 0.9\n",
    "    m, n = sim_matrix.shape\n",
    "    forward = np.eye(n)[sim_matrix.argmax(axis=1)]  # m x n\n",
    "    backward = np.eye(m)[sim_matrix.argmax(axis=0)]  # n x m\n",
    "    inter = forward * backward.transpose()\n",
    "\n",
    "    if min(m, n) <= 2:\n",
    "        return inter\n",
    "\n",
    "    new_inter = np.zeros((m, n))\n",
    "    count = 1\n",
    "    while count < max_count:\n",
    "        mask_x = 1.0 - np.tile(inter.sum(1)[:, np.newaxis], (1, n)).clip(0.0, 1.0)\n",
    "        mask_y = 1.0 - np.tile(inter.sum(0)[np.newaxis, :], (m, 1)).clip(0.0, 1.0)\n",
    "        mask = ((alpha_ratio * mask_x) + (alpha_ratio * mask_y)).clip(0.0, 1.0)\n",
    "        mask_zeros = 1.0 - ((1.0 - mask_x) * (1.0 - mask_y))\n",
    "        if mask_x.sum() < 1.0 or mask_y.sum() < 1.0:\n",
    "            mask *= 0.0\n",
    "            mask_zeros *= 0.0\n",
    "\n",
    "        new_sim = sim_matrix * mask\n",
    "        fwd = np.eye(n)[new_sim.argmax(axis=1)] * mask_zeros\n",
    "        bac = np.eye(m)[new_sim.argmax(axis=0)].transpose() * mask_zeros\n",
    "        new_inter = fwd * bac\n",
    "\n",
    "        if np.array_equal(inter + new_inter, inter):\n",
    "                break\n",
    "        inter = inter + new_inter\n",
    "        count += 1\n",
    "        \n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5aafd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the CSV file containing the eng text\n",
    "NH_eng_sentences = pd.read_csv(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/NH_eng_groupedsentences.csv\")\n",
    "## open the CSV file containing the lat text\n",
    "NH_lat_sentences = pd.read_csv(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/NH_lat_groupedsentences.csv\", dtype={'reference': 'str'})\n",
    "NH_lat_sentences['loc_ent_id'] = '-' ## the column will contain the id of the LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d918f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/sentence_alignment/sentence_pairs_alignment/sentence_pairs_alignment\"\n",
    "pattern = '\\[(\\d+(?:,\\s*\\d+)*)\\](?:,\\s*\\[(\\d+(?:,\\s*\\d+)*)\\])?.*$'\n",
    "\n",
    "for book_n in range(1,38): ## for each book\n",
    "    \n",
    "    file_name = path+str(book_n)+'.txt' \n",
    "    \n",
    "    filter_book_eng = NH_eng_sentences[NH_eng_sentences['book'] == book_n] ## select the tokens in the eng book\n",
    "    filter_book_lat = NH_lat_sentences[NH_lat_sentences['book'] == book_n] ## select the tokens in the lat book\n",
    "    \n",
    "    with open(file_name, \"r\") as file: ## open the file containing the sentence alignments for the eng and lat book\n",
    "        \n",
    "        for line in file: ## for each alignment\n",
    "            match1 = re.search(pattern,line) ## get the index pair of the alignment\n",
    "            \n",
    "            index_eng = match1[1] ## get the index(es) of the english sentence(s)\n",
    "            index_lat = match1[2] ## get the index(es) of the latin sentence(s)\n",
    "            \n",
    "            if index_eng and index_lat: ## if an alignment was found\n",
    "                                \n",
    "                index_eng = index_eng.split(',') ## if there is more than one index, split them\n",
    "                index_eng = [int(i) for i in index_eng]\n",
    "                filter_sentence_eng = filter_book_eng[filter_book_eng['sentence'].isin(index_eng)] ## select all the tokens in the corresponding sentence(s)\n",
    "                filter_sentence_eng = filter_sentence_eng.reset_index()\n",
    "                                                       \n",
    "                src_sent = []\n",
    "                indexes_to_project = []\n",
    "                loc_ent_ids = []\n",
    "                \n",
    "                for i, engtoken in enumerate(filter_sentence_eng['token']): ## for each token\n",
    "                    src_sent.append(engtoken) ## list the tokens of the source eng sentence\n",
    "                    loc_ent_ids.append(filter_sentence_eng['loc_ent_id'][i]) ## list the loc_ent_id for each token\n",
    "                    \n",
    "                    if 'LOC' in filter_sentence_eng['flair_ner'][i]: ## if the token is a LOC entity\n",
    "                        indexes_to_project.append(i) ## keep the index\n",
    "                \n",
    "                if len(indexes_to_project) > 0: ## if the sentence contains a LOC entity\n",
    "                                        \n",
    "                    index_lat = index_lat.split(',')\n",
    "                    index_lat = [int(i) for i in index_lat]\n",
    "                    filter_sentence_lat = filter_book_lat[filter_book_lat['sentence'].isin(index_lat)]\n",
    "                    filter_sentence_lat = filter_sentence_lat.reset_index()\n",
    "                    \n",
    "                    trg_sent = []\n",
    "                    indexes_to_update = []\n",
    "                    \n",
    "                    for i, lattoken in enumerate(filter_sentence_lat['token']):\n",
    "                        trg_sent.append(lattoken)  ## list the tokens of the target lat sentence\n",
    "                        indexes_to_update.append(filter_sentence_lat['level_0'][i]) ## index of the token in the NH_lat_sentences dataframe\n",
    "                    \n",
    "                    ## perform the word alignment (code revisited from SimAlign)\n",
    "                    \n",
    "                    l1_tokens = [tokenizer.tokenize(word) for word in src_sent] \n",
    "                    l2_tokens = [tokenizer.tokenize(word) for word in trg_sent] \n",
    "                    bpe_lists = [[bpe for w in sent for bpe in w] for sent in [l1_tokens, l2_tokens]]\n",
    "    \n",
    "                    l1_b2w_map = []\n",
    "                    for i, wlist in enumerate(l1_tokens):\n",
    "                        l1_b2w_map += [i for x in wlist]\n",
    "    \n",
    "                    l2_b2w_map = []\n",
    "                    for i, wlist in enumerate(l2_tokens):\n",
    "                        l2_b2w_map += [i for x in wlist]\n",
    "                    \n",
    "                    vectors = get_embed_list([src_sent, trg_sent]).cpu().detach().numpy()\n",
    "                    vectors = [vectors[i, :len(bpe_lists[i])] for i in [0, 1]]\n",
    "\n",
    "                    all_mats = {} ## create a dictionary\n",
    "                    sim = get_similarity(vectors[0], vectors[1]) ## get the cosine similarity\n",
    "                    all_mats[\"itermax\"] = iter_max(sim) ## generate a key-value\n",
    "                    aligns = {} ## create a dictionary\n",
    "                    aligns['itermax'] = set()\n",
    "\n",
    "                    for i in range(len(vectors[0])):\n",
    "                        for j in range(len(vectors[1])):\n",
    "                            if all_mats['itermax'][i, j] > 0:\n",
    "                                aligns['itermax'].add((l1_b2w_map[i], l2_b2w_map[j]))\n",
    "                \n",
    "                    aligns['itermax'] = sorted(aligns['itermax']) ## word pairs\n",
    "                                        \n",
    "                    for index_to_project in indexes_to_project: ## for each index to project\n",
    "                        \n",
    "                        projection = 0\n",
    "                        label_to_project = flair_annotation[index_to_project]\n",
    "                        ent_loc_id = loc_ent_ids[index_to_project]\n",
    "                        topostext_id = topostext_ids[index_to_project]\n",
    "                        \n",
    "                        ## project the LOC ids\n",
    "                        \n",
    "                        for wordpair in aligns['itermax']: ## for each word alignment in the sentences\n",
    "                            i_engword = int(wordpair[0]) ## get the index of the eng word\n",
    "                            if index_to_project == i_engword: ## if the index was aligned\n",
    "                                i_latword = int(wordpair[1]) ## get the index of the lat word\n",
    "                                \n",
    "                                NH_lat_sentences['loc_ent_id'][indexes_to_update[i_latword]] = ent_loc_id ## indexes_to_update[i_latword] gets the index of the token in the NH_lat_sentences dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2f689",
   "metadata": {},
   "source": [
    "Assign the IOB labels B-LOC and I-LOC to the Latin tokens containing the projected loc_ent_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "NH_lat_loc_ent =  NH_lat_sentences[NH_lat_sentences['loc_ent_id'] != '-'] ## select all the LOC tokens\n",
    "NH_lat_loc_ent.reset_index()\n",
    "max_value = NH_lat_loc_ent['loc_ent_id'].max() ## get the maximum numeric value of the loc ids\n",
    "\n",
    "for n in range(0, max_value+1): ## for each loc_ent_id\n",
    "    \n",
    "    filter_loc_ent = NH_lat_loc_ent[NH_lat_loc_ent['loc_ent_id'] == n] ## select all the tokens with the same loc_ent_id\n",
    "    filter_loc_ent.reset_index(inplace=True)\n",
    "    for i,flair_ner in enumerate(filter_loc_ent['flair_ner']): ## for each token\n",
    "        index_to_update = filter_loc_ent['level_0'][i]\n",
    "        if i == 0: ## for the first token\n",
    "            NH_lat_sentences['flair_ner'][index_to_update] = 'B-LOC' ## assign B-LOC\n",
    "        else : NH_lat_sentences['flair_ner'][index_to_update] = 'I-LOC' ## assign I-LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9903af",
   "metadata": {},
   "outputs": [],
   "source": [
    "NH_lat_sentences.to_csv('NH_lat_projected.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
